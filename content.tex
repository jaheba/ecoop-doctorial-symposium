% \clearpage

\section{Introduction}

For over 60 years, \emph{just-in-time} (JIT) (or dynamic) compilation techniques
have been used in software systems. They are used to improve time and space
efficiency in software applications~\cite{aycock2003brief}. Although JITs don't
add functionality to a system, they are a crucial element of influential
software projects such as Lisp interpreters, Adaptive Fortran, Smalltalk, SELF,
and Java Virtual Machines (JVMs) (c.f.~\cite{arnold2005survey}). Recently, the
rapid spread of web applications relied -- among others -- on JIT-compiling
virtual machines (VMs) for browser-side JavaScript.

The usage of term `just-in-time' to refer to dynamic compilation only became
popular in the 1990s. Originally used to describe a methodology in the car
manufacturing industry, it came into fashion with the rise of Java. Whilst
originally it might have been used to describe simple translation of VM
instructions (v-code) to native machine-code instructions (n-code) `just' before
execution (hence just-in-time), it nowadays generally describes dynamic
compilation systems.

Some VMs can compete with statically compiled and optimised languages in
benchmarks. W\"urthinger et al.~\cite{wurthinger2013one} characterise these VMs
as \emph{high-performance} if they are able to execute applications ``within a
small multiple (1-3x)'' of the time taken by equivalent statically compiled
code.

However, developing and maintaining a JIT infrastructure for a high-performance
VM entails high costs, which only a few organisations can afford. ``Until
industrial-scale investment becomes available, the complexity of a traditional
high-performance VM is too high for small-scale efforts to build and maintain a
high-performance implementation for multiple hardware platforms and operating
systems.''~\cite{wurthinger2013one}
Therefore, only a few programming languages have high-performance
implementations. Worse, often multiple implementation projects target the same
language -- for example Java (Oracle's HotSpot VM, IBM's J9 VM) and JavaScript
(e.g.~Google's V8, Mozilla's SpiderMonkey).

Creators and implementors of new or competing programming languages face a
dilemma, as Tratt~\cite{tratt_fast_enough} describes: ``how much implementation
is needed to show that a language design is good?''. Tratt argues that if too
little effort is spent on a performant implementation a ``language will be
dismissed out of hand as unusably slow'', but if too much work is put into the
implementation instead of the design, then ``low-level implementation concerns
can easily end up dictating higher-level design choices''.

Attempts to directly re-target sophisticated VMs for other languages have been
made -- with varying success w.r.t.~performance. Bolz and
Tratt~\cite{bolz14impact}~assert that ``a VM reflects the language, or group of
languages, it was designed for. [...] If not, the \emph{semantic mismatch}
between the two leads to poor performance for user programs.'' For example,
implementations of the Python language for the JVM (Jython) and CLR (IronPython)
weren't able to provide significant performance improvements over Python's
default (non-optimising) interpreter.~\cite{bolz14impact}.

A relatively new trend are tool-sets for \emph{constructing} high-performance
language runtimes. Under the name of \emph{Eclipse
OMR}~\cite{gaudet2016rebuilding}, IBM has open-sourced components of their
industrial-strength JVM implementation J9 -- including \emph{JitBuilder}.
Eclipse OMR has separated ``parts that implement Java semantics from the parts
that provide key runtime capabilities''~\cite{eclipseOMR} and offers them as a
runtime technology platform.

A different approach is to derive dynamic compilers from the description of a
language-interpreter using the
\emph{Truffle}~\cite{wurthinger2013one} or
\emph{RPython/PyPy}~\cite{bolz2009tracing} projects. Language implementors are
responsible for writing an interpreter for their target language, utilising a
provided toolchain. So-created runtimes employ JIT-compilers which can translate
user programs to native-code by the means of \emph{dynamic partial evaluation}
(Truffle) or meta-tracing (RPython). The benefit of these approaches is that
language implementors don't directly interact with the dynamic-compilation
system, but instead guide it through annotations in the source code of the
newly written interpreters.

Marr and Ducasse~\cite{marr2015tracing} conducted a  comparison of Truffle and
RPython by implementing VMs for SOM, a dynamic object-oriented language. They
observed that both implementations reached ``roughly the same level of
performance``, and stayed in one order of magnitude compared to the JVM. They
argue, that ``neither of the two approaches has a fundamental advantage for the
reached peak-performance. [...] With respect to the engineering, tracing has
however significant benefits, because it requires language implementers to apply
fewer optimizations to reach the same level of performance.''

In comparison to other dynamic compilation systems, meta-traced VMs have
relatively lackluster warmup characteristics, meaning that it takes longer to
generate and execute efficient and performant native
code.~\cite{barrett2016virtual} In my research I want to improve the warmup
behaviour of meta-tracing VMs.


\section{Warmup and Meta-Tracing}

Two general approaches have been established for JIT-compilation: method and
trace based systems. Method based JITs are roughly akin to static compilers and compile
functions as units of code. In contrast, tracing focuses on a dynamic form of
program representation and traces are derived from observing execution of
loop-bodies. The compiled code-fragments represent the control flow, which was
taken during execution of the loop-body. Tracing usually operates on top of an
existing interpreter and significant speedups can be accomplished quickly.
Method based JITs require more tuning, which complicates design and
implementation of such systems, but also offer overall more fine-grained
control.

Tracing systems associate counters with the program position (PC) of
loop-headers. Each time a loop is encountered, its counter is incremented. After
a threshold value is reached, recording begins and executed v-code instruction
are collected. When execution reaches the loop's header again, recording stops
and the collected instructions can be compiled to n-code. The next time
the VM reaches the same loop-header, it can execute the compiled trace -- which
is identified by the pc of the loop-header -- instead.

Meta-tracing describes the execution of a guest-VM on top of a tracing
host-VM. The host employs a JIT-compiler, which is able to optimise the
execution of the guest-VM. Through this layered execution, the host-VM thereby also
traces and optimises the user's program. Since the host-VM has no understanding
of the user's program, the guest-VM has to provide hints to indicate loops
within the user's program. Without these hints the host-VM's JIT would only
identify loops of the guest-VM. Each execution step of the user's program in the
host-VM can be identified by an \emph{abstract}-pc, a tuple consisting of the
host-VM's pc and guest-VM's pc (compare~\cite{sullivan2003dynamic}). However for
tracing purposes, only the guest-VM's pc is essential.



% \section{Problem description}

% It has been shown, that meta-tracing is an efficient way of writing jit-compiled
% high-performance virtual machines (VM). \cite{marr2015tracing}

% Explain JITs? What it is?

% * What are VMs
% * What is high-performance and why does it matter

% * How can high-performance be achieved
%     - jit / dynamic compilation



% Thus, performance is always wanted / needed / hard to achieve -> leads into dilemma.



% \subsection{Fast VMs In Fast Enough Time}


% \clearpage

\subsection{Warmup}

When an optimising VM starts executing a program, it does so by executing a
general, unoptimised version of said program. During this phase, the VM profiles
the program and starts creating specialised and optimised code for frequently
executed `hot' parts of the program. These newly compiled fragments can then be
used instead of the slower initial representation. This phase -- the time before
fast dynamically compiled code is executed -- is often referred to as warmup.
It is said that a VM has reached a steady state of peak performance once it has
finished warmup.

Most systems use counters to determine whether a unit of code should be
considered as cold or hot. Tracing-based VMs typically count how often a loop-body has
been executed and start tracing a given loop, when a threshold value has been
reached. When choosing a good threshold value, opportunity costs for simply
continue running the program have to be considered; the benefit
of creating optimised code has to amortise its own creation costs. Language
implementations with smaller opportunity costs use lower threshold values. For
instance, LuaJIT's fast tracer has a threshold of just 56 iterations while
RPython's meta tracer uses
1039 cycles \laurie{help the reader and say ``because luajit is ...'',
preferably with a citation}.

Meta-traced VMs inherently \laurie{if it's inherent, you can't fix it ;) so i'd say ``currently'' or similar} have bad warmup-behaviour. In RPython based systems,
user-programs are executed with a slow profiling-interpreter first. After a hot
loop has been identified, it is meta-traced -- an interpreter for the host
language is running the interpreter for the guest language. This phase is about
200x slower than the already slow profiling-interpreter.

The high overhead of meta-traced VMs to perform dynamic compilation impact their
warmup characteristics twofold. The slow meta-tracing phase enforces a high
threshold value, to ensure that compilation costs are likely amortised. To
improve warmup the penalty for JIT-compilation has to be reduced, which would
reflect in lower threshold values. One approach is to utilise parallelism to
reduce the risk of needless compilation efforts.


% Although the
% tracer has to observe actual execution, some task can be outsourced to different
% workers. For example compilation can happen in separate, dedicated threads.

% But first understanding \laurie{something disappeared here?}

% % tracer has to see the world as it was
% advantage is that we have a static system we trace

\section{Approach}

When investigating warmup characteristics and attempting to improve them it is
important to understand the improvement limitations.



 Put differently, how much does warmup actually
affect overall runtime performance.

\laurie{also need an evaluation section. some mention of the languages you might implement, the benchmarks you might use and how
you will record performance (e.g.~using kalibera's method or the VM warmup blows hot and cold approach)}

\subsection{Baseline}


Kulkarni~\cite{kulkarni2011jit} conducted a study about the impact of modern
multi-core machines on VM performance. He compared the impact of different
threshold values and number of compilation threads on the Java HotSpot across
several standard benchmarks. 

he has fully static as base line

fully compiled needs 9\% of time of pure interpretation
30\% improvement if everything was compiled over default threshold



How much overhead is actually imposed by jit-compilation?

used standard benchmarks


\subsubsection{Understand Meta-Tracing Warmup}

Although it can be safely said that meta-tracing VMs generally have bad warmup
characteristics, a closer look is required. What could be achieved?

\laurie{at the moment this sounds a bit like ``here's some engineering'' -- maybe phrase them more in terms of research challenges. ok, parallel morphing is too obvious for this to be worth doing. but say with basic blocks you could say something like ``Research question: can recording basic blocks make the overall performance better than recording individual instructions? My hypothesis is that the increase in latency will be offset by an increase in throughput''.}

\jasper{write about rust}

\subsection{Improvements}

Just three of many possible

\subsubsection{Parallel Morphing} Current meta-tracing system's single threaded
architecture disallows parallel execution of interpreter and the tracing system.
Thus, time spent in collecting, translating and optimising traces is ``lost'',
which could be used for advancing program-state instead (opportunity costs).
De-coupling compilation efforts from normal execution of user-programs could
potentially reduce the performance penalty significantly.



What is theoretically possible, assuming it works.

What changes are necessary, can it practically work?

Evaluating this approach requires a working system, which can 

Before potential performance benefits or trade-offs can measured, 



examine this question, 

What are the requirements to allow this?


\subsubsection{Basic-Block Trace Collection} Present tracing-solutions derive
traces by collecting executed v-code instructions. To reduce load from the
actual tracing process, basic-blocks could be collected instead. When the trace
is later translated into a native representation, information about executed
basic blocks is sufficient to restore control which was taken during execution.
This idea is based on the parallel morphing approach, as it assumes that
compilation of traces happens in parallel to normal interpreter execution.

Basic-block tracing increases the latency before compiled native-code is
available.

% Although, it does not decrease the overall workload, it reduces
% the time consumed during tracing.

\subsubsection{Different Optimisation Levels}

one size fits all


Given a collected trace, there is a large set of optimisations which could be
conducted on that trace. To achieve earlier execution of
optimised code, multiple optimisation levels could be used. For example,
lightly optimised, medium-quality code can be generated and used quickly, whilst
high-performance n-code is generated in parallel.

This could have two effects. For one, the VM can warmup faster, since optimised
code is available sooner. For two, the pressure to quickly generate high quality
n-code is lower, because `fastish' code is already running. Thus, this could not
only improve warmup behaviour, but improve peak performance.



% \clearpage
